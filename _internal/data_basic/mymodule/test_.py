import sys
import os
import time
import requests
from PyQt5.QtTest import *
import variable as v_

sys.path.append('C:/my_games/' + str(v_.game_folder) + '/' + str(v_.data_folder) + '/mymodule')

my_newsapi = "ad1a0e63885745ffbee8a5258bda298a"


def go_test(keyword):
    import numpy as np
    import cv2
    import pyautogui
    import random
    from bs4 import BeautifulSoup
    import re
    import requests
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    import time
    import requests
    import re
    import urllib.parse
    from news_economy import news_economy_start
    from drama_review import drama_start
    from env_ai import wp_id, wp_ps, keys
    from life_tips import life_tips_keyword, check_openai_ready, suggest_life_tip_topic
    from trend_search_page import trend_search

    try:
        print("test")
        cla = "one"

        plus = 0


        if cla == "one":
            plus = 0
        elif cla == "two":
            plus = 960
        elif cla == "three":
            plus = 960 * 2
        elif cla == "four":
            plus = 960 * 3
        elif cla == "five":
            plus = 960 * 4
        elif cla == "six":
            plus = 960 * 5


        # suggest_life_tip_topic()

        print("v_.my_prompt", v_.my_prompt)

        print(sys.executable)

        # trend_search()





    except Exception as e:
        print(e)


def stable_diff():
    import requests
    import base64

    # 1. ì´ë¯¸ì§€ ìƒì„± ìš”ì²­
    payload = {
        "prompt": "a fantasy castle at sunset, highly detailed, artstation style",
        "negative_prompt": "blurry, low quality, bad anatomy",
        "steps": 20,
        "width": 512,
        "height": 512,
        "sampler_index": "Euler",
        "cfg_scale": 7.5
    }

    response = requests.post("http://127.0.0.1:7860/sdapi/v1/txt2img", json=payload)

    # 2. base64 ë¬¸ìì—´ ì¶”ì¶œ ë° ë””ì½”ë”©
    image_base64 = response.json()['images'][0]
    image_data = base64.b64decode(image_base64)

    # 3. PNG ì´ë¯¸ì§€ë¡œ ì €ì¥
    with open("output.png", "wb") as f:
        f.write(image_data)

    print("âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: output.png")

    #############################################
#
def check_quota():
    import openai
    try:
        usage = openai.API().usage()  # ì‹¤ì œ quota í™•ì¸ì€ OpenAI Dashboard APIë¡œ êµ¬í˜„ í•„ìš”
        print(f"ğŸ” ì‚¬ìš©ëŸ‰ ì§„ë‹¨: {usage}")
    except Exception as e:
        print(f"âš ï¸ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨ ë˜ëŠ” API ì œí•œ: {e}")


def post_test():
    from env_ai import wp_id, wp_ps
    from wordpress_xmlrpc import Client, WordPressPost
    from wordpress_xmlrpc.methods.posts import NewPost

    try:
        print("â–¶ post_test ì‹œì‘")

        username = wp_id()
        password = wp_ps()
        site_url = "https://hobbycolorful.com/xmlrpc.php"

        # 1) í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = Client(site_url, username, password)

        # 2) í¬ìŠ¤íŠ¸ ê°ì²´ ìƒì„±
        post = WordPressPost()

        # ğŸ“° ì œëª©Â·ë³¸ë¬¸ ì„¤ì •
        post.title       = "ì—¬ê¸°ëŠ” ê¸€ì˜ ì œëª©ì…ë‹ˆë‹¤"
        post.content     = "ì—¬ê¸°ëŠ” ê¸€ì˜ ë‚´ìš©ì…ë‹ˆë‹¤"
        post.post_status = "publish"  # ê³µê°œ ë°œí–‰

        # ğŸ¯ ì¹´í…Œê³ ë¦¬ ì§€ì • (ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê·¸ëŒ€ë¡œ, ê´„í˜¸ í¬í•¨)
        post.terms_names = {
            'category': ['HobbyWhite(ë‰´ìŠ¤)']
        }

        # 3) í¬ìŠ¤íŠ¸ ì—…ë¡œë“œ
        client.call(NewPost(post))
        print("âœ… í¬ìŠ¤íŒ…ì´ HobbyWhite(ë‰´ìŠ¤) ì¹´í…Œê³ ë¦¬ì— ì—…ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)

def naver_news():
    import numpy as np
    import cv2
    import pyautogui
    import random
    from bs4 import BeautifulSoup
    import re
    import requests
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    import time
    import requests
    import re
    import urllib.parse

    try:
        print("test")
        cla = "one"

        plus = 0


        if cla == "one":
            plus = 0
        elif cla == "two":
            plus = 960
        elif cla == "three":
            plus = 960 * 2
        elif cla == "four":
            plus = 960 * 3
        elif cla == "five":
            plus = 960 * 4
        elif cla == "six":
            plus = 960 * 5


        print("ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤")

        headers = {
            "User-Agent": "Mozilla/5.0"
        }

        # 1. êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ "ë„¤ì´ë²„ ê²½ì œ" ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
        query = "ë„¤ì´ë²„ ê²½ì œ site:news.naver.com"
        google_news_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&tbm=nws&hl=ko"

        res = requests.get(google_news_url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")

        # 2. êµ¬ê¸€ ë§í¬ì—ì„œ ì‹¤ì œ ë„¤ì´ë²„ ë‰´ìŠ¤ ì£¼ì†Œë§Œ ì¶”ì¶œ
        def extract_real_url(google_href):
            match = re.search(r"/url\?q=(https?://[^&]+)", google_href)
            if match:
                return urllib.parse.unquote(match.group(1))
            return None

        article_links = []
        for a in soup.select("a[href^='/url?q=']"):
            real_url = extract_real_url(a.get("href"))
            if real_url and "news.naver.com" in real_url:
                article_links.append(real_url)

        article_links = list(set(article_links))[:5]  # ìƒìœ„ 5ê°œë§Œ

        print("âœ… ì •ì œëœ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬:")
        for link in article_links:
            print(link)

        # 3. ê° ê¸°ì‚¬ì—ì„œ ì œëª©/ë³¸ë¬¸/GPT ìš”ì•½ ì¶”ì¶œ
        results = []

        for url in article_links:
            try:
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.text, "html.parser")

                # ì œëª© ì¶”ì¶œ
                title_tag = soup.select_one("h2#title_area span")
                title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

                # ë³¸ë¬¸ ì¶”ì¶œ
                content_tag = soup.select_one("#dic_area")
                if not content_tag:
                    print(f"âŒ ë³¸ë¬¸ ì—†ìŒ: {url}")
                    continue

                content = content_tag.get_text(separator=" ", strip=True)
                content = re.sub(r"\s+", " ", content)

                # ê°„ë‹¨í•œ GPT ìŠ¤íƒ€ì¼ ìš”ì•½ ìƒì„± (ë£° ê¸°ë°˜)
                summary = f"ì´ ê¸°ì‚¬ëŠ” '{title}'ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ, ì£¼ìš” ê²½ì œ ì´ìŠˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
                if "ê¸ˆë¦¬" in content or "ì¸ìƒ" in content:
                    summary += "ğŸ“ˆ ê¸ˆë¦¬ ì •ì±…ì´ ì£¼ìš” ìŸì ì´ë©°, ì¤‘ì•™ì€í–‰ì˜ ê²°ì •ì´ ê¸ˆìœµì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤."
                elif "ë¬¼ê°€" in content or "ì¸í”Œë ˆì´ì…˜" in content:
                    summary += "ğŸ’¹ ì¸í”Œë ˆì´ì…˜ê³¼ ë¬¼ê°€ ë³€í™”ê°€ í•µì‹¬ ì´ìŠˆë¡œ, ì†Œë¹„ì ê²½ì œì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤."
                elif "í™˜ìœ¨" in content:
                    summary += "ğŸ’± í™˜ìœ¨ ë³€ë™ì´ ê¸€ë¡œë²Œ ë¬´ì—­ê³¼ ìˆ˜ì¶œì…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ë¶„ì„ë˜ê³  ìˆìŠµë‹ˆë‹¤."
                elif "ë¬´ì—­" in content or "ìˆ˜ì¶œ" in content:
                    summary += "ğŸš¢ ë¬´ì—­ìˆ˜ì§€ì™€ ìˆ˜ì¶œì… ë³€í™”ê°€ ê±°ì‹œê²½ì œì— ì£¼ëŠ” ì˜í–¥ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤."
                else:
                    summary += "ğŸ“Š ë‹¤ì–‘í•œ ê²½ì œ ì§€í‘œì™€ ì •ì±… ë³€í™”ê°€ ë…¼ì˜ë˜ê³  ìˆìœ¼ë©°, í–¥í›„ ì‹œì¥ íë¦„ì´ ì£¼ëª©ë©ë‹ˆë‹¤."

                # ê²°ê³¼ ì €ì¥
                results.append({
                    "title": title,
                    "summary": summary,
                    "content": content
                })

                print(f"âœ… '{title}' ê¸°ì‚¬ ìš”ì•½ ì™„ë£Œ")

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # 4. ì¶œë ¥
        print(f"\nğŸ“¦ ìµœì¢… ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°œìˆ˜: {len(results)}")
        for i, r in enumerate(results, 1):
            print(f"\nğŸ“° ë‰´ìŠ¤ {i}")
            print(f"ì œëª©: {r['title']}")
            print(f"\nâœï¸ ìš”ì•½:\n{r['summary']}")
            print(f"\nğŸ“„ ë³¸ë¬¸:\n{r['content'][:500]}...")

    except Exception as e:
        print(e)

def daum_news():
    import numpy as np
    import cv2
    import pyautogui
    import random
    from bs4 import BeautifulSoup
    import re
    import requests
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    import time
    import requests
    import re
    import urllib.parse

    try:
        print("test")
        cla = "one"

        plus = 0


        if cla == "one":
            plus = 0
        elif cla == "two":
            plus = 960
        elif cla == "three":
            plus = 960 * 2
        elif cla == "four":
            plus = 960 * 3
        elif cla == "five":
            plus = 960 * 4
        elif cla == "six":
            plus = 960 * 5


        print("ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤")

        import feedparser

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        # 1. ì—°í•©ë‰´ìŠ¤TV ê²½ì œ RSS íŒŒì‹±
        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        RSS_URL = "http://www.yonhapnewstv.co.kr/category/news/economy/feed/"
        feed = feedparser.parse(RSS_URL)

        # ì •ìƒ ì‘ë‹µ í™•ì¸
        if feed.bozo:
            raise RuntimeError("RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìƒìœ„ 5ê°œ í•­ëª© ê°€ì ¸ì˜¤ê¸°
        entries = feed.entries[:5]

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        # 2. ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì œëª©Â·ë§í¬Â·ì„¤ëª… ì €ì¥
        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        results = []
        for entry in entries:
            results.append({
                'title': entry.get('title', '').strip(),
                'url': entry.get('link', '').strip(),
                'description': entry.get('summary', '').strip()
            })

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        # 3. ê²°ê³¼ ì¶œë ¥
        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        print(f"ğŸ“¦ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(results)}\n")
        for i, item in enumerate(results, 1):
            print(f"=== ë‰´ìŠ¤ {i} ===")
            print(f"ì œëª©       : {item['title']}")
            print(f"ë§í¬       : {item['url']}")
            print(f"ê°„ëµì„¤ëª…   : {item['description']}\n")

    except Exception as e:
        print(e)


def openai_api_test__():

    from openai import OpenAI
    from dotenv import load_dotenv
    from env_ai import keys

    client = OpenAI(api_key = keys())
    load_dotenv()


    try:
        print("test")
        cla = "one"

        plus = 0


        if cla == "one":
            plus = 0
        elif cla == "two":
            plus = 960
        elif cla == "three":
            plus = 960 * 2
        elif cla == "four":
            plus = 960 * 3
        elif cla == "five":
            plus = 960 * 4
        elif cla == "six":
            plus = 960 * 5


        print("Opneai")

        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": "ë„ˆëŠ” ëˆ„êµ¬ì•¼?"
                }
            ]
        )

        print(completion.choices[0].message.content)

    except Exception as e:
        print(e)

