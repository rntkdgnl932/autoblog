import os
import json
import re
import requests
from bs4 import BeautifulSoup
import variable as v_

ORG_DB_PATH = "C:/my_games/auto_blog/mysettings/idpw/my_list.json"

# âœ… 1. ê¸°ê´€ëª… ìë™ ì¶”ì¶œ í•¨ìˆ˜
def my_organization_list(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    text = soup.get_text()

    raw_candidates = re.findall(r"([\w\uAC00-\uD7AF]{2,10}(ì²­|ë¶€|ì²˜|ê³µë‹¨|ê³µì‚¬|ìœ„ì›íšŒ|ì›|ì„¼í„°|ê¸°ê¸ˆ|ì¬ë‹¨|ì§€ì›ë‹¨|í˜‘íšŒ|ì§€ì›ì„¼í„°))", text)
    cleaned = set()

    for org, _ in raw_candidates:
        if not re.search(r"[0-9]|[^\w\uAC00-\uD7AF]", org):
            cleaned.add(org.strip())

    return list(cleaned)

# âœ… 2. Google Custom Searchë¡œ ëŒ€í‘œë²ˆí˜¸ ë° í™ˆí˜ì´ì§€ ê²€ìƒ‰

def scan_internet(org_name):
    if not org_name:
        return

    try:
        if not os.path.exists(ORG_DB_PATH):
            os.makedirs(os.path.dirname(ORG_DB_PATH), exist_ok=True)
            with open(ORG_DB_PATH, 'w', encoding='utf-8') as f:
                json.dump({}, f, ensure_ascii=False, indent=2)

        try:
            with open(ORG_DB_PATH, 'r', encoding='utf-8') as f:
                org_data = json.load(f)
        except json.JSONDecodeError:
            backup_path = ORG_DB_PATH + ".bak"
            os.rename(ORG_DB_PATH, backup_path)
            print(f"[!] JSON ì˜¤ë¥˜ ë°œìƒ - ë°±ì—… ì €ì¥: {backup_path}")
            org_data = {}

        print(f"[WEB] ê²€ìƒ‰ì¤‘: {org_name}")

        # ğŸ” 1ì°¨ ê²€ìƒ‰: í™ˆí˜ì´ì§€
        query_home = f"{org_name}"
        resp_home = requests.get(
            "https://www.googleapis.com/customsearch/v1",
            params={
                "key": v_.my_google_custom_api,
                "cx": v_.my_google_custom_id,
                "q": query_home,
                "num": 3
            }
        )
        results_home = resp_home.json()
        site_url = ""
        for item in results_home.get("items", []):
            link = item.get("link", "")
            if re.search(r"https?://[\w\.-]*\.(go\.kr|or\.kr|co\.kr|ac\.kr|re\.kr|ne\.kr|pe\.kr|com|net|org|edu|kr)", link):
                site_url = link
                break

        # ğŸ” 2ì°¨ ê²€ìƒ‰: ì „í™”ë²ˆí˜¸
        query_phone = f"{org_name} ëŒ€í‘œë²ˆí˜¸"
        resp_phone = requests.get(
            "https://www.googleapis.com/customsearch/v1",
            params={
                "key": v_.my_google_custom_api,
                "cx": v_.my_google_custom_id,
                "q": query_phone,
                "num": 3
            }
        )
        results_phone = resp_phone.json()
        phone = ""
        for item in results_phone.get("items", []):
            snippet = item.get("snippet", "")
            phone_match = re.search(r"(0\d{1,2}-\d{3,4}-\d{4})", snippet)
            if phone_match:
                phone = phone_match.group(1)
                break

        # âœ… ì €ì¥ (ê¸°ì¡´ê°’ë„ ë®ì–´ì“°ê¸°)
        if phone and site_url:
            org_data[org_name] = {"ì „í™”": phone, "ì‚¬ì´íŠ¸": site_url}
            print(f"[+] ì €ì¥ì™„ë£Œ: {org_name} ({phone}, {site_url})")
        else:
            print(f"[-] ê²€ìƒ‰ ì‹¤íŒ¨: {org_name} - phone: {phone}, url: {site_url}")

        with open(ORG_DB_PATH, 'w', encoding='utf-8') as f:
            json.dump(org_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] {org_name} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

# âœ… 3. HTMLì—ì„œ ê¸°ê´€ëª… ê´€ë ¨ ì „í™”/ì‚¬ì´íŠ¸ë¥¼ ìë™ ëŒ€ì²´ (ì¤‘ë³µ h2 ì œê±° í¬í•¨)
def last_upload_ready(html_content):
    if not os.path.exists(ORG_DB_PATH):
        return html_content

    with open(ORG_DB_PATH, 'r', encoding='utf-8') as f:
        org_data = json.load(f)

    # ğŸ” ì¤‘ë³µ <h2> ì œê±°
    soup = BeautifulSoup(html_content, 'html.parser')
    seen_h2 = set()
    for h2 in soup.find_all("h2"):
        text = h2.get_text(strip=True)
        if text in seen_h2:
            h2.decompose()
        else:
            seen_h2.add(text)

    html_content = str(soup)

    # ğŸ§  GPT ìµœì¢…ê²€ìˆ˜ë¥¼ ìœ„í•œ íŒ¨í„´ ê¸°ë°˜ êµì²´
    def replace_info(match):
        full_text = match.group(0)
        for org, info in org_data.items():
            if org in full_text:
                result = full_text
                if re.search(r"ì „í™”|ëŒ€í‘œë²ˆí˜¸|ê³ ê°ì„¼í„°", full_text):
                    result = f"{org} ëŒ€í‘œì „í™”: {info['ì „í™”']}"
                if re.search(r"í™ˆí˜ì´ì§€|ì‚¬ì´íŠ¸|ê³µì‹ í™ˆí˜ì´ì§€", full_text):
                    result = f'{org} (<a href="{info["ì‚¬ì´íŠ¸"]}" target="_blank" rel="noopener">ê³µì‹ í™ˆí˜ì´ì§€</a>)'
                return result
        return full_text

    pattern = r"([\w\uAC00-\uD7AF]{2,10}(ì²­|ë¶€|ì²˜|ê³µë‹¨|ê³µì‚¬|ìœ„ì›íšŒ|ì›|ì„¼í„°|ê¸°ê¸ˆ|ì¬ë‹¨|ì§€ì›ë‹¨|í˜‘íšŒ|ì§€ì›ì„¼í„°))[^<\n]{0,40}(ì „í™”|ëŒ€í‘œë²ˆí˜¸|ê³ ê°ì„¼í„°|í™ˆí˜ì´ì§€|ì‚¬ì´íŠ¸|ê³µì‹ í™ˆí˜ì´ì§€)"
    html_content = re.sub(pattern, replace_info, html_content)

    return html_content
